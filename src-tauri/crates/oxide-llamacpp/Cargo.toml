[package]
name = "llamacpp"
version = "0.6.599"
authors = ["Oxide Lab Contributors"]
description = "Tauri plugin for managing llama.cpp server processes and model loading"
license = "MIT"
repository = "https://github.com/FerrisMind/Oxide-Lab"
edition = "2021"
rust-version = "1.77.2"
exclude = ["/examples", "/dist-js", "/guest-js", "/node_modules"]
links = "llamacpp"

[dependencies]
base64 = "0.22.1"
byteorder = "1.5.0"
hmac = "0.12.1"
oxide-utils = { path = "../oxide-utils" }
log = "0.4"
rand = "0.8"
serde = { version = "1.0", features = ["derive"] }
sha2 = "0.10.9"
sysinfo = "0.34.2"
tauri = { version = "2.5.0", default-features = false, features = [] }
thiserror = "2.0.12"
tokio = { version = "1", features = ["full"] }
oxide-hardware = { package = "hardware", path = "../oxide-hardware" }
reqwest = { version = "0.11", features = ["json", "blocking", "stream", "native-tls"] }
flate2 = "1.1.0"
tar = "0.4.44"
zip = { version = "2.2.2", default-features = false, features = ["deflate"] }

# Unix-specific dependencies
[target.'cfg(unix)'.dependencies]
nix = { version = "=0.30.1", features = ["signal", "process"] }

[dev-dependencies]
tempfile = "3.0"

[build-dependencies]
tauri-plugin = { version = "2.3.1", features = ["build"] }
